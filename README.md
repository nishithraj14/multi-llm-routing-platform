# ğŸ§  Multi-LLM Routing Platform

A production-grade orchestration platform that dynamically routes user prompts across multiple Large Language Models (LLMs) based on prompt complexity, priority, cost, and performance â€” with benchmarking, evaluation scoring, and real-time UI visualization.

---

## ğŸš€ Live System Capabilities

* ğŸ”€ Intelligent prompt routing
* ğŸ§­ Priority-aware model selection
* ğŸ§ª Multi-model benchmark execution
* ğŸ“Š Cost vs latency comparison
* ğŸ§® Response quality evaluation
* ğŸ›Ÿ Automatic fallback handling
* âš¡ Streaming response rendering
* ğŸ–¥ï¸ FastAPI dashboard UI
* ğŸ§© Local + API model orchestration

---

## ğŸ—ï¸ Architecture Overview

```
User Prompt
     â†“
FastAPI UI Layer
     â†“
Routing Engine
     â†“
Model Selection Logic
     â†“
LLM Execution Layer
   â†™        â†“        â†˜
OpenAI   Mistral   Ollama (Local)
     â†“
Evaluation Engine
     â†“
Cost + Latency Metrics
     â†“
Benchmark Comparator
     â†“
Streaming Dashboard Output
```

---

## ğŸ§  Routing Intelligence

The router selects the optimal model based on:

| Factor              | Description                |
| ------------------- | -------------------------- |
| Prompt complexity   | Coding vs general queries  |
| Priority mode       | Quality / Speed / Balanced |
| Latency sensitivity | Faster model routing       |
| Cost awareness      | Token pricing              |
| Benchmark toggle    | Multi-model execution      |

---

## ğŸ¯ Priority Modes

| Mode         | Behavior                                    |
| ------------ | ------------------------------------------- |
| **Quality**  | Routes to highest reasoning model (OpenAI)  |
| **Speed**    | Routes to fastest inference model (Mistral) |
| **Balanced** | Smart heuristic routing                     |

---

## ğŸ¤– Supported Models

### Cloud Providers

* OpenAI (GPT-4o)
* Mistral Large

### Local Models (via Ollama)

* LLaMA3
* Mistral
* Phi

Hybrid orchestration supported.

---

## ğŸ“Š Benchmark Mode

Executes all models in parallel and compares:

| Metric  | Measured         |
| ------- | ---------------- |
| Latency | Execution time   |
| Cost    | Token pricing    |
| Score   | Response quality |
| Output  | Model answer     |

Best model selected automatically.

---

## ğŸ§® Evaluation Engine

Responses are scored heuristically on:

* Length adequacy
* Structural completeness
* Code presence
* Clarity indicators

Score range:

```
0 â†’ 10
```

Used for benchmark ranking & fallback triggers.

---

## ğŸ›Ÿ Fallback Logic

If a model:

* Fails execution
* Times out
* Produces low score

System falls back:

```
OpenAI â†’ Mistral â†’ Ollama
```

Ensures reliability.

---

## ğŸ–¥ï¸ UI Dashboard Features

* Prompt playground
* Priority selector
* Benchmark toggle
* Streaming response rendering
* Model selection badge
* Cost / latency / score tiles
* Benchmark comparison table
* Cost vs latency chart

Server-rendered via FastAPI + Jinja.

---

## ğŸ“ Project Structure

```
backend/
â”‚
â”œâ”€â”€ main.py                # FastAPI entrypoint
â”œâ”€â”€ ui_routes.py           # UI + execution routes
â”œâ”€â”€ config.py              # Environment config
â”‚
â”œâ”€â”€ router/
â”‚   â””â”€â”€ model_router.py
â”‚
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”œâ”€â”€ mistral_provider.py
â”‚   â””â”€â”€ ollama_provider.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluator.py
â”‚
â”œâ”€â”€ cost/
â”‚   â””â”€â”€ cost_calculator.py
â”‚
â”œâ”€â”€ fallback/
â”‚   â””â”€â”€ fallback_engine.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ latency.py
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ result.html
â”‚
â””â”€â”€ static/
    â”œâ”€â”€ styles.css
    â””â”€â”€ icons/
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/nishithraj14/multi-llm-routing-platform.git
cd multi-llm-routing-platform

python -m venv venv
venv\Scripts\activate

pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

Create `.env`:

```
OPENAI_API_KEY=your_key
MISTRAL_API_KEY=your_key
OLLAMA_BASE_URL=http://localhost:11434
```

---

## â–¶ï¸ Run Platform

```bash
uvicorn backend.main:app --reload
```

Open:

```
http://127.0.0.1:8000
```

---

## ğŸ§ª Example Workflows

### Single Routing

Prompt â†’ Routed to best model â†’ Evaluated â†’ Displayed

---

### Benchmark Execution

Prompt â†’ All models run â†’ Compared â†’ Best selected

---

## ğŸ“Š Sample Metrics Output

| Model   | Latency | Cost    | Score |
| ------- | ------- | ------- | ----- |
| OpenAI  | 8.5s    | $0.0029 | 7     |
| Mistral | 4.1s    | $0.0012 | 6     |
| Ollama  | 111s    | $0.00   | 7     |

---

## ğŸ› ï¸ Tech Stack

**Backend**

* FastAPI
* LiteLLM
* Python
* Requests

**LLM Providers**

* OpenAI API
* Mistral API
* Ollama Local

**UI**

* Jinja2 Templates
* Chart.js
* Streaming JS Renderer

---

## ğŸ“Œ Use Cases

* Cost-aware LLM routing
* Multi-vendor orchestration
* Model benchmarking
* Latency optimization
* Prompt complexity classification

---

## ğŸ§‘â€ğŸ’» Author

**Nishith Raj**

AI Engineering â€¢ LLM Systems â€¢ Model Orchestration

GitHub:
[https://github.com/nishithraj14](https://github.com/nishithraj14)

---

## â­ Project Value

This project demonstrates:

* LLM infra orchestration design
* Multi-provider abstraction
* Evaluation pipelines
* Cost optimization routing
* Hybrid local + API inference
* Production UI exposure

---

## ğŸ“œ License

MIT License
